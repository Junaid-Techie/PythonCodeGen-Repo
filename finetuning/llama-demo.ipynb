{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6143af52-8340-49f8-994d-4e6442c9cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"30,31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ecd05-10d0-4178-ba71-98e0375f7020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b713ee7b-b9aa-4129-bb89-5871364d68c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-23 23:18:08.909694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745464688.919675 1835973 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745464688.922785 1835973 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-23 23:18:08.939335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/students/stalakol/.local/lib/python3.10/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "saved_directory = \"./saved-models/llama/\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_directory,load_in_4bit=True, torch_dtype=torch.float16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c2eda-4ef8-4485-91c7-ec08b6fcdc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69461d82-8a9b-4270-85e6-d5a62d8ba50d",
   "metadata": {},
   "source": [
    "### BERT + `accelerator` Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99d1110-062c-4e99-860d-bb00ac2d8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Create a BERT model using huggingface module and write the backpropagation code using accelerator for multi-GPU training\n",
      "\n",
      "\n",
      "### Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Create a BERT model using huggingface module and write the backpropagation code using accelerator for multi-GPU training\"\n",
    "\n",
    "prompt = f\"### Instruction:\\n{question}\\n\\n### Output:\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca1b357-f932-461e-8b57-862a319fbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a6ada6-4c2e-4d00-b3fb-5a4dc87d538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Create a BERT model using huggingface module and write the backpropagation code using accelerator for multi-GPU training\n",
      "\n",
      "\n",
      "### Output:\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from transformers import BertModel, BertConfig\n",
      "\n",
      "class BertModelWithAccelerator(nn.Module):\n",
      "    def __init__(self, config):\n",
      "        super().__init__()\n",
      "        self.bert = BertModel(config)\n",
      "        self.out_layer = nn.Linear(config.hidden_size, config.hidden_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.bert(x)\n",
      "        return self.out_layer(out)\n",
      "\n",
      "bert_config = BertConfig.from_pretrained('bert-base-uncased')\n",
      "bert_model = BertModelWithAccelerator(bert_config)\n",
      "\n",
      "# Training\n",
      "for epoch in range(10):\n",
      "    train_loss = 0\n",
      "    train_acc = 0\n",
      "    for i, (input, target) in enumerate(train_dataloader):\n",
      "        output = bert_model(input)\n",
      "        loss = F.cross_entropy(output, target)\n",
      "        train_loss += loss.item()\n",
      "        train_acc += F.sigmoid(output).max(1)[1]\n",
      "\n",
      "    print('Epoch %d: loss: %.3f, accuracy: %.3f' % (epoch, train_loss / len(train_dataloader), train_acc / len(train_dataloader)))\n",
      "\n",
      "# Evaluation\n",
      "for i, (input, target) in enumerate(test_dataloader):\n",
      "    output = bert_model(input)\n",
      "    loss = F.cross_entropy(output, target)\n",
      "    test_loss += loss.item()\n",
      "    test_acc += F.sigmoid(output).max(1)[1]\n",
      "\n",
      "print('Test loss: %.3f, accuracy: %.3f' % (test_loss / len(test_dataloader), test_acc / len(test_dataloader)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8813fe-9848-4e08-8a5a-257df5f706b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fd005cc-ef60-483a-8146-9f77aa6ec720",
   "metadata": {},
   "source": [
    "### Easy Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe5bb24-533b-4456-8aef-7dabcc42feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Create a function to calculate the sum of a sequence of integers\n",
      "\n",
      "\n",
      "### Output:\n",
      "\n",
      "def sum_sequence(sequence):\n",
      "    sum = 0\n",
      "    for num in sequence:\n",
      "        sum += num\n",
      "    return sum\n",
      "\n",
      "\n",
      "sequence = [1, 2, 3, 4]\n",
      "print(sum_sequence(sequence)) # 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Create a function to calculate the sum of a sequence of integers\"\n",
    "\n",
    "prompt = f\"### Instruction:\\n{question}\\n\\n### Output:\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "\n",
    "print(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf8cf4-c80a-4874-92c9-a75df7b6d8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7320f571-12bd-41c1-8ca6-1394d24f92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Create a function to calculate the sum of a sequence of floats\n",
      "\n",
      "### Output:\n",
      "\n",
      "def sum_sequence(sequence):\n",
      "    return sum(sequence)\n",
      "\n",
      "print(sum_sequence([1, 2, 3, 4, 5])) # 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Create a function to calculate the sum of a sequence of floats\"\n",
    "\n",
    "prompt = f\"### Instruction:\\n{question}\\n\\n### Output:\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "\n",
    "print(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61569c-2844-43bd-9f17-6d09eaad0891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cae15da1-bc42-4bae-96c9-ea885fc7c977",
   "metadata": {},
   "source": [
    "### Easy Question with little bit details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47aaed96-23bb-4660-ae83-75ebf48d2aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Create a BERT model using the huggingface module\n",
      "\n",
      "\n",
      "### Output:\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "from transformers import BertModel, BertConfig\n",
      "from transformers import BertTokenizer\n",
      "\n",
      "# Create the model\n",
      "model = BertModel.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Create the config\n",
      "config = BertConfig.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Create the tokenizer\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Create the model\n",
      "model = BertModel.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Create the config\n",
      "config = BertConfig.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Create the tokenizer\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Create a BERT model using the huggingface module\"\n",
    "\n",
    "prompt = f\"### Instruction:\\n{question}\\n\\n### Output:\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "\n",
    "print(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a051e-86c9-4aa0-9433-8ff8b0d47397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
